{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76aafe0",
   "metadata": {},
   "source": [
    "# Matrix Factorisation with Drug and Target similarity\n",
    "Currently using cosine similarity for the target similarity matrix but this is being changed to a normalised smith waterman score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d3d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tdc.multi_pred import DTI\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import logging\n",
    "from rdkit import Chem\n",
    "\n",
    "# load in the three datasets\n",
    "data_Kd = DTI(name = 'BindingDB_Kd')\n",
    "data_Kd.convert_to_log(form = 'binding')\n",
    "\n",
    "# data_ic50 = DTI(name = 'BindingDB_IC50')\n",
    "# data_ic50.convert_to_log(form = 'binding')\n",
    "\n",
    "# data_Ki = DTI(name = 'BindingDB_Ki')\n",
    "# data_Ki.convert_to_log(form = 'binding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b09be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data):\n",
    "    # split data and get ID dicts\n",
    "    split = data.get_split(seed = 42,  frac = [0.6, 0.05, 0.35])\n",
    "    train = split['train']\n",
    "    test = split['test']\n",
    "\n",
    "    train = train[['Drug_ID', 'Drug', 'Target', 'Y']].dropna()\n",
    "    train = train.reset_index(drop=True)\n",
    "\n",
    "    ID_to_Drug = dict(enumerate(list(dict.fromkeys(train['Drug_ID']))))\n",
    "    ID_to_Target = dict(enumerate(list(dict.fromkeys(train['Target']))))\n",
    "    Drug_to_ID = dict((v,k) for k,v in ID_to_Drug.items())\n",
    "    Target_to_ID = dict((v,k) for k,v in ID_to_Target.items())\n",
    "    \n",
    "    return train, test, Drug_to_ID, Target_to_ID\n",
    "\n",
    "def data_loader(data, drug_dict, target_dict):\n",
    "    # load data into correct format\n",
    "    data[\"Target_ID2\"] = data[\"Target\"].apply(lambda x:target_dict.get(x))\n",
    "    data[\"Drug_ID2\"] = data[\"Drug_ID\"].apply(lambda x:drug_dict.get(x))\n",
    "    data = data.dropna()\n",
    "\n",
    "    drug_ID = data[\"Drug_ID2\"].to_numpy()\n",
    "    target_ID = data[\"Target_ID2\"].to_numpy()\n",
    "    features = np.vstack((drug_ID, target_ID)).T\n",
    "    label = data['Y'].to_numpy()\n",
    "    return features, label\n",
    "\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, train, label):\n",
    "        self.feature_= train\n",
    "        self.label_= label\n",
    "    def __len__(self):\n",
    "    #return size of dataset\n",
    "        return len(self.feature_)\n",
    "    def __getitem__(self, idx):\n",
    "        return  torch.tensor(self.feature_[idx], dtype=torch.long),torch.tensor(self.label_[idx], dtype=torch.float)\n",
    "    \n",
    "    \n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_factors, drug_sim_mat, target_sim_mat):\n",
    "        super().__init__()\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors)\n",
    "        torch.nn.init.xavier_uniform_(self.user_factors.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.item_factors.weight)\n",
    "        \n",
    "        self.user_biases = torch.nn.Embedding(n_users, 1)\n",
    "        self.item_biases = torch.nn.Embedding(n_items,1)\n",
    "        self.user_biases.weight.data.fill_(0.)\n",
    "        self.item_biases.weight.data.fill_(0.)\n",
    "        \n",
    "        # NEW WEIGHTS FOR THE SIMILARITY MATRIX\n",
    "        self.drug_sim = drug_sim_mat\n",
    "        self.user_sim = torch.nn.Embedding(n_users, 1)\n",
    "        self.target_sim = target_sim_mat\n",
    "        self.item_sim = torch.nn.Embedding(n_items, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.user_sim.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.item_sim.weight)\n",
    "\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_len = len(user)\n",
    "        item_len = len(item)\n",
    "        \n",
    "        AAT_list = [torch.dot(self.user_factors(user)[i,:], self.user_factors(user)[i,:]) for i in range(self.user_factors(user).shape[0])]\n",
    "        AAT = torch.tensor(AAT_list)\n",
    "        \n",
    "        BBT_list = [torch.dot(self.item_factors(item)[i,:], self.item_factors(item)[i,:]) for i in range(self.item_factors(item).shape[0])]\n",
    "        BBT = torch.tensor(BBT_list)\n",
    "        \n",
    "        pred = self.user_biases(user) + self.item_biases(item)\n",
    "        pred += (self.user_factors(user) * self.item_factors(item)).sum(1, keepdim=True)\n",
    "        \n",
    "        # Sd = A*AT\n",
    "        pred += ((self.drug_sim[user][:,0] * self.user_sim(user).double().reshape(user_len)) - AAT).reshape(user_len,1)\n",
    "        # St = B*BT\n",
    "        pred += ((self.target_sim[item][:,0] * self.item_sim(item).double().reshape(item_len)) - BBT).reshape(item_len,1)\n",
    "        \n",
    "        return pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beaabc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, test_loader, model, num_epochs=100):\n",
    "#     dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    # need to change dtype to float to reduce resources required\n",
    "    # maybe just one csim_drug.to(dev)\n",
    "    dev = torch.device(\"cpu\")\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    \n",
    "    model.to(dev)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(0,num_epochs):\n",
    "        count = 0\n",
    "        cum_loss = 0.\n",
    "        for i, (train_batch, label_batch) in enumerate(train_loader):\n",
    "            count = 1 + i\n",
    "            # Predict and calculate loss for user factor and bias\n",
    "            #### WEIGHTS FOR USER SIMILARITY MATRIX ADDED INTO OPTIMISER HERE\n",
    "            optimizer = torch.optim.SGD([model.user_biases.weight,model.user_factors.weight,\n",
    "                                         model.user_sim.weight], lr=0.05, weight_decay=1e-3)\n",
    "            prediction = model(train_batch[:,0].to(dev), train_batch[:,1].to(dev))\n",
    "            loss = loss_func(prediction, label_batch.to(dev)).float()    \n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # predict and calculate loss for item factor and bias\n",
    "            #### WEIGHTS FOR ITEM SIMILARITY MATRIX ADDED INTO OPTIMISER HERE\n",
    "            optimizer = torch.optim.SGD([model.item_biases.weight,model.item_factors.weight,\n",
    "                                    model.item_sim.weight], lr=0.05, weight_decay=1e-3)              \n",
    "            prediction = model(train_batch[:,0].to(dev), train_batch[:,1].to(dev))\n",
    "            loss = loss_func(prediction, label_batch.to(dev))\n",
    "            loss_item = loss.item()\n",
    "            cum_loss += loss_item\n",
    "\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_loss = cum_loss/count\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        cum_loss =0.\n",
    "        count = 0\n",
    "        for i, (test_batch, label_batch) in enumerate(test_loader):\n",
    "            count = 1 + i\n",
    "            with torch.no_grad():\n",
    "                prediction = model(test_batch[:,0].to(dev), test_batch[:,1].to(dev))\n",
    "                loss = loss_func(prediction, label_batch.to(dev))\n",
    "                cum_loss += loss.item()\n",
    "\n",
    "        test_loss = cum_loss/count\n",
    "        test_losses.append(test_loss)\n",
    "        if epoch % 1 == 0:\n",
    "            print('epoch: ', epoch ,' avg training loss: ', train_loss, ' avg test loss: ',test_loss)\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b8b31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_model(data, csim_drug, img_name, n_factors=100, bs=100, num_epochs=100):\n",
    "    train, test, drug_dict, target_dict = data_split(data)\n",
    "    x_train, y_train = data_loader(train, drug_dict, target_dict)\n",
    "    x_test, y_test = data_loader(test, drug_dict, target_dict)\n",
    "\n",
    "    train_dataloader = DataLoader(RatingDataset(x_train, y_train), batch_size=bs, shuffle=True)\n",
    "    test_dataloader = DataLoader(RatingDataset(x_test, y_test), batch_size=bs)\n",
    "    \n",
    "    csim_target_np = cos_matrix(train, 'Target', False)\n",
    "    csim_target = torch.from_numpy(csim_target_np)\n",
    "\n",
    "    \n",
    "    model = MatrixFactorization(len(drug_dict), len(target_dict), n_factors, csim_drug, csim_target)\n",
    "\n",
    "    train_losses, test_losses = train_model(train_dataloader, test_dataloader, model, num_epochs)\n",
    "\n",
    "    epochs = range(1, num_epochs+1)\n",
    "    plt.plot(epochs, train_losses, label='train')\n",
    "    plt.plot(epochs, test_losses, label='test')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('mse loss')\n",
    "    plt.legend()\n",
    "    plt.title(img_name)\n",
    "#     plt.savefig(img_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade7b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kd_drug_sim_np = np.loadtxt('../sim_matrix/drug_sim.txt', delimiter=',')\n",
    "Kd_drug_sim = torch.from_numpy(Kd_drug_sim_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9f650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARPROTSET = {\n",
    "    \"A\": 1,\n",
    "    \"C\": 2,\n",
    "    \"B\": 3,\n",
    "    \"E\": 4,\n",
    "    \"D\": 5,\n",
    "    \"G\": 6,\n",
    "    \"F\": 7,\n",
    "    \"I\": 8,\n",
    "    \"H\": 9,\n",
    "    \"K\": 10,\n",
    "    \"M\": 11,\n",
    "    \"L\": 12,\n",
    "    \"O\": 13,\n",
    "    \"N\": 14,\n",
    "    \"Q\": 15,\n",
    "    \"P\": 16,\n",
    "    \"S\": 17,\n",
    "    \"R\": 18,\n",
    "    \"U\": 19,\n",
    "    \"T\": 20,\n",
    "    \"W\": 21,\n",
    "    \"V\": 22,\n",
    "    \"Y\": 23,\n",
    "    \"X\": 24,\n",
    "    \"Z\": 25,\n",
    "}\n",
    "\n",
    "CHARPROTLEN = 25\n",
    "\n",
    "CHARISOSMISET = {\n",
    "    \"#\": 29,\n",
    "    \"%\": 30,\n",
    "    \")\": 31,\n",
    "    \"(\": 1,\n",
    "    \"+\": 32,\n",
    "    \"-\": 33,\n",
    "    \"/\": 34,\n",
    "    \".\": 2,\n",
    "    \"1\": 35,\n",
    "    \"0\": 3,\n",
    "    \"3\": 36,\n",
    "    \"2\": 4,\n",
    "    \"5\": 37,\n",
    "    \"4\": 5,\n",
    "    \"7\": 38,\n",
    "    \"6\": 6,\n",
    "    \"9\": 39,\n",
    "    \"8\": 7,\n",
    "    \"=\": 40,\n",
    "    \"A\": 41,\n",
    "    \"@\": 8,\n",
    "    \"C\": 42,\n",
    "    \"B\": 9,\n",
    "    \"E\": 43,\n",
    "    \"D\": 10,\n",
    "    \"G\": 44,\n",
    "    \"F\": 11,\n",
    "    \"I\": 45,\n",
    "    \"H\": 12,\n",
    "    \"K\": 46,\n",
    "    \"M\": 47,\n",
    "    \"L\": 13,\n",
    "    \"O\": 48,\n",
    "    \"N\": 14,\n",
    "    \"P\": 15,\n",
    "    \"S\": 49,\n",
    "    \"R\": 16,\n",
    "    \"U\": 50,\n",
    "    \"T\": 17,\n",
    "    \"W\": 51,\n",
    "    \"V\": 18,\n",
    "    \"Y\": 52,\n",
    "    \"[\": 53,\n",
    "    \"Z\": 19,\n",
    "    \"]\": 54,\n",
    "    \"\\\\\": 20,\n",
    "    \"a\": 55,\n",
    "    \"c\": 56,\n",
    "    \"b\": 21,\n",
    "    \"e\": 57,\n",
    "    \"d\": 22,\n",
    "    \"g\": 58,\n",
    "    \"f\": 23,\n",
    "    \"i\": 59,\n",
    "    \"h\": 24,\n",
    "    \"m\": 60,\n",
    "    \"l\": 25,\n",
    "    \"o\": 61,\n",
    "    \"n\": 26,\n",
    "    \"s\": 62,\n",
    "    \"r\": 27,\n",
    "    \"u\": 63,\n",
    "    \"t\": 28,\n",
    "    \"y\": 64,\n",
    "}\n",
    "\n",
    "CHARISOSMILEN = 64\n",
    "\n",
    "CHARATOMSET = [\n",
    "    \"C\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"S\",\n",
    "    \"F\",\n",
    "    \"Si\",\n",
    "    \"P\",\n",
    "    \"Cl\",\n",
    "    \"Br\",\n",
    "    \"Mg\",\n",
    "    \"Na\",\n",
    "    \"Ca\",\n",
    "    \"Fe\",\n",
    "    \"As\",\n",
    "    \"Al\",\n",
    "    \"I\",\n",
    "    \"B\",\n",
    "    \"V\",\n",
    "    \"K\",\n",
    "    \"Tl\",\n",
    "    \"Yb\",\n",
    "    \"Sb\",\n",
    "    \"Sn\",\n",
    "    \"Ag\",\n",
    "    \"Pd\",\n",
    "    \"Co\",\n",
    "    \"Se\",\n",
    "    \"Ti\",\n",
    "    \"Zn\",\n",
    "    \"H\",\n",
    "    \"Li\",\n",
    "    \"Ge\",\n",
    "    \"Cu\",\n",
    "    \"Au\",\n",
    "    \"Ni\",\n",
    "    \"Cd\",\n",
    "    \"In\",\n",
    "    \"Mn\",\n",
    "    \"Zr\",\n",
    "    \"Cr\",\n",
    "    \"Pt\",\n",
    "    \"Hg\",\n",
    "    \"Pb\",\n",
    "    \"Unknown\",\n",
    "]\n",
    "\n",
    "CHARATOMLEN = 44\n",
    "\n",
    "\n",
    "def integer_label_smiles(smiles, max_length=85, isomeric=False):\n",
    "    \"\"\"\n",
    "    Integer encoding for SMILES string sequence.\n",
    "    Args:\n",
    "        smiles (str): Simplified molecular-input line-entry system, which is a specification in the form of a line\n",
    "        notation for describing the structure of chemical species using short ASCII strings.\n",
    "        max_length (int): Maximum encoding length of input SMILES string. (default: 85)\n",
    "        isomeric (bool): Whether the input SMILES string includes isomeric information (default: False).\n",
    "    \"\"\"\n",
    "    if not isomeric:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            logging.warning(f\"rdkit cannot find this SMILES {smiles}.\")\n",
    "            return None\n",
    "        smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True)\n",
    "    encoding = np.zeros(max_length)\n",
    "    for idx, letter in enumerate(smiles[:max_length]):\n",
    "        try:\n",
    "            encoding[idx] = CHARISOSMISET[letter]\n",
    "        except KeyError:\n",
    "            logging.warning(\n",
    "                f\"character {letter} does not exists in default SMILE category encoding, skip and treat as \" f\"padding.\"\n",
    "            )\n",
    "\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def integer_label_protein(sequence, max_length=1200):\n",
    "    \"\"\"\n",
    "    Integer encoding for protein string sequence.\n",
    "    Args:\n",
    "        sequence (str): Protein string sequence.\n",
    "        max_length: Maximum encoding length of input protein string. (default: 1200)\n",
    "    \"\"\"\n",
    "    encoding = np.zeros(max_length)\n",
    "    for idx, letter in enumerate(sequence[:max_length]):\n",
    "        try:\n",
    "            encoding[idx] = CHARPROTSET[letter]\n",
    "        except KeyError:\n",
    "            logging.warning(\n",
    "                f\"character {letter} does not exists in sequence category encoding, skip and treat as \" f\"padding.\"\n",
    "            )\n",
    "    return encoding\n",
    "\n",
    "def cos_matrix(train, col, Drug):\n",
    "#     sentences = []\n",
    "    sentences_comb = []\n",
    "    for i in range(len(train[col])):\n",
    "        if Drug == True:\n",
    "            num_rep = integer_label_smiles(train[col][i]).astype(str)\n",
    "            num_rep_trim = np.trim_zeros(num_rep)\n",
    "        else:\n",
    "            num_rep = integer_label_protein(train[col][i]).astype(str)\n",
    "            num_rep_trim = np.trim_zeros(num_rep)\n",
    "\n",
    "#         sentences.append(num_rep)\n",
    "\n",
    "        str_rep = \" \".join(num_rep_trim)\n",
    "        sentences_comb.append(str_rep)\n",
    "    vectorizer = CountVectorizer().fit_transform(sentences_comb)\n",
    "    vectors = vectorizer.toarray()\n",
    "    csim = cosine_similarity(vectors)\n",
    "    return csim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024d14f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  avg training loss:  28.34370138083294  avg test loss:  18.83685381753104\n",
      "epoch:  1  avg training loss:  18.08625157471675  avg test loss:  11.842932510375977\n",
      "epoch:  2  avg training loss:  13.270707974767989  avg test loss:  8.463146441323417\n",
      "epoch:  3  avg training loss:  10.685226458652764  avg test loss:  6.646307563781738\n",
      "epoch:  4  avg training loss:  9.141301947794142  avg test loss:  5.581969445092337\n",
      "epoch:  5  avg training loss:  8.134556414974723  avg test loss:  4.909772838865008\n",
      "epoch:  6  avg training loss:  7.428863048553467  avg test loss:  4.454018531526838\n",
      "epoch:  7  avg training loss:  6.895751418581434  avg test loss:  4.125686373029437\n",
      "epoch:  8  avg training loss:  6.474533287582884  avg test loss:  3.874986226218087\n",
      "epoch:  9  avg training loss:  6.128798679181725  avg test loss:  3.675952236992972\n",
      "epoch:  10  avg training loss:  5.839500393837121  avg test loss:  3.5115288870675223\n",
      "epoch:  11  avg training loss:  5.590548703624944  avg test loss:  3.3729645047869\n",
      "epoch:  12  avg training loss:  5.370656188126582  avg test loss:  3.2531529494694302\n",
      "epoch:  13  avg training loss:  5.17803973756778  avg test loss:  3.1486564295632498\n",
      "epoch:  14  avg training loss:  5.003266102189471  avg test loss:  3.055895011765616\n",
      "epoch:  15  avg training loss:  4.848270369183486  avg test loss:  2.9729290536471775\n",
      "epoch:  16  avg training loss:  4.705077118175045  avg test loss:  2.8974327087402343\n",
      "epoch:  17  avg training loss:  4.5743919800800885  avg test loss:  2.8279343707220894\n",
      "epoch:  18  avg training loss:  4.453740655996238  avg test loss:  2.765299776622227\n",
      "epoch:  19  avg training loss:  4.3439217661596405  avg test loss:  2.706876048019954\n",
      "epoch:  20  avg training loss:  4.239610410799646  avg test loss:  2.6523145624569486\n",
      "epoch:  21  avg training loss:  4.1434418365454215  avg test loss:  2.6016847031457084\n",
      "epoch:  22  avg training loss:  4.0534686905563255  avg test loss:  2.553885429246085\n",
      "epoch:  23  avg training loss:  3.9695441935472426  avg test loss:  2.509260140146528\n",
      "epoch:  24  avg training loss:  3.8881368955988793  avg test loss:  2.4668772424970355\n",
      "epoch:  25  avg training loss:  3.812729296410919  avg test loss:  2.4261316878455026\n",
      "epoch:  26  avg training loss:  3.740661892162007  avg test loss:  2.3872539826801846\n",
      "epoch:  27  avg training loss:  3.672098211422088  avg test loss:  2.3505110366003854\n",
      "epoch:  28  avg training loss:  3.60681822193656  avg test loss:  2.3152062995093208\n",
      "epoch:  29  avg training loss:  3.544212110482963  avg test loss:  2.2815308485712325\n",
      "epoch:  30  avg training loss:  3.4839131072827967  avg test loss:  2.248777106830052\n",
      "epoch:  31  avg training loss:  3.426015847807477  avg test loss:  2.217497159753527\n",
      "epoch:  32  avg training loss:  3.3704504556716626  avg test loss:  2.1869813323020937\n",
      "epoch:  33  avg training loss:  3.316473903170057  avg test loss:  2.1575358492987498\n",
      "epoch:  34  avg training loss:  3.2653834151614243  avg test loss:  2.129770953314645\n",
      "epoch:  35  avg training loss:  3.2166148583600473  avg test loss:  2.102690073422023\n",
      "epoch:  36  avg training loss:  3.1670570563358864  avg test loss:  2.0763765709740776\n",
      "epoch:  37  avg training loss:  3.120609041991507  avg test loss:  2.0504664352961948\n",
      "epoch:  38  avg training loss:  3.077426520122844  avg test loss:  2.0254337787628174\n",
      "epoch:  39  avg training loss:  3.032389955156168  avg test loss:  2.001068297454289\n",
      "epoch:  40  avg training loss:  2.9902953008177935  avg test loss:  1.9784226383481707\n",
      "epoch:  41  avg training loss:  2.9500395560720163  avg test loss:  1.9555286645889283\n",
      "epoch:  42  avg training loss:  2.9102983421580806  avg test loss:  1.9335591179983957\n",
      "epoch:  43  avg training loss:  2.873048065574306  avg test loss:  1.9116148386682783\n",
      "epoch:  44  avg training loss:  2.8352687670167085  avg test loss:  1.8910901410239083\n",
      "epoch:  45  avg training loss:  2.7980614270374273  avg test loss:  1.8733341063771929\n",
      "epoch:  46  avg training loss:  2.765434820181245  avg test loss:  1.8526637434959412\n",
      "epoch:  47  avg training loss:  2.7329440063731685  avg test loss:  1.8340166909354074\n",
      "epoch:  48  avg training loss:  2.700901225873619  avg test loss:  1.8151757529803685\n",
      "epoch:  49  avg training loss:  2.6681115369128574  avg test loss:  1.798522881099156\n",
      "epoch:  50  avg training loss:  2.6390717910353545  avg test loss:  1.7807732258524214\n",
      "epoch:  51  avg training loss:  2.609074316207011  avg test loss:  1.7653903416224888\n",
      "epoch:  52  avg training loss:  2.581802141134906  avg test loss:  1.7493300199508668\n",
      "epoch:  53  avg training loss:  2.5542201130253495  avg test loss:  1.7336891685213363\n",
      "epoch:  54  avg training loss:  2.5278760749063673  avg test loss:  1.7196080463273185\n",
      "epoch:  55  avg training loss:  2.5024987937538485  avg test loss:  1.7046615617615837\n",
      "epoch:  56  avg training loss:  2.477171708823769  avg test loss:  1.6925855875015259\n",
      "epoch:  57  avg training loss:  2.4538188086953134  avg test loss:  1.6793959021568299\n",
      "epoch:  58  avg training loss:  2.4303446947389347  avg test loss:  1.6668483444622584\n",
      "epoch:  59  avg training loss:  2.408123820450655  avg test loss:  1.6537245239530292\n",
      "epoch:  60  avg training loss:  2.3857508373867935  avg test loss:  1.643240123135703\n",
      "epoch:  61  avg training loss:  2.366902182056646  avg test loss:  1.6324270095144\n",
      "epoch:  62  avg training loss:  2.3467069279616046  avg test loss:  1.6226516434124538\n",
      "epoch:  63  avg training loss:  2.328241718043188  avg test loss:  1.6173041428838457\n",
      "epoch:  64  avg training loss:  2.31369018630617  avg test loss:  1.6171990036964417\n",
      "epoch:  65  avg training loss:  2.309014610424163  avg test loss:  1.6634861775806973\n",
      "epoch:  66  avg training loss:  2.489082027392782  avg test loss:  2.9062645401273457\n",
      "epoch:  67  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  68  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  69  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  70  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  71  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  72  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  73  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  74  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  75  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  76  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  77  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  78  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  79  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  80  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  81  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  82  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  83  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  84  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  85  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  86  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  87  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  88  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  89  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  90  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  91  avg training loss:  nan  avg test loss:  nan\n",
      "epoch:  92  avg training loss:  nan  avg test loss:  nan\n"
     ]
    }
   ],
   "source": [
    "full_model(data_Kd, Kd_drug_sim, 'Kd', n_factors=30, bs=200, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc8312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
